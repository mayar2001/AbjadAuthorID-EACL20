{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14447775,"sourceType":"datasetVersion","datasetId":9228680},{"sourceId":14448404,"sourceType":"datasetVersion","datasetId":9229043}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-09T21:03:53.559457Z","iopub.execute_input":"2026-01-09T21:03:53.559793Z","iopub.status.idle":"2026-01-09T21:03:53.969176Z","shell.execute_reply.started":"2026-01-09T21:03:53.559766Z","shell.execute_reply":"2026-01-09T21:03:53.967946Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/authorid-training/AuthorshipClassficiationTrain.xlsx\n/kaggle/input/authorid/AuthorshipClassficiationVal.xlsx\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport os\n\n# 1. Automatic detection of the uploaded files\n# Based on your uploads, we search for Train and Val files\ntrain_path = \"\"\nval_path = \"\"\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        full_path = os.path.join(dirname, filename)\n        if 'train' in filename.lower():\n            train_path = full_path\n        elif 'val' in filename.lower():\n            val_path = full_path\n\n# 2. Loading and Converting\ntry:\n    if train_path and val_path:\n        # Load Excel files\n        train_df = pd.read_excel(train_path)\n        val_df = pd.read_excel(val_path)\n        \n        # Save as CSV with correct encoding for Arabic\n        train_df.to_csv('train_data.csv', index=False, encoding='utf-8-sig')\n        val_df.to_csv('val_data.csv', index=False, encoding='utf-8-sig')\n        \n        print(\"Status: Success\")\n        print(f\"Loaded Train: {train_path} ({len(train_df)} rows)\")\n        print(f\"Loaded Val: {val_path} ({len(val_df)} rows)\")\n        print(\"-\" * 30)\n        print(\"New CSV files created: 'train_data.csv' and 'val_data.csv'\")\n    else:\n        print(\"Status: Error - Could not find the files. Please check if they are added to the notebook.\")\nexcept Exception as e:\n    print(f\"Status: Exception - {str(e)}\")\n\n# 3. Preview to verify column names\nif 'train_df' in locals():\n    print(\"\\nColumn names found:\", train_df.columns.tolist())\n    print(\"\\nFirst 2 rows of Training Data:\")\n    print(train_df.head(2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T21:03:53.971004Z","iopub.execute_input":"2026-01-09T21:03:53.971563Z","iopub.status.idle":"2026-01-09T21:04:02.135425Z","shell.execute_reply.started":"2026-01-09T21:03:53.971530Z","shell.execute_reply":"2026-01-09T21:04:02.134567Z"}},"outputs":[{"name":"stdout","text":"Status: Success\nLoaded Train: /kaggle/input/authorid-training/AuthorshipClassficiationTrain.xlsx (35122 rows)\nLoaded Val: /kaggle/input/authorid/AuthorshipClassficiationVal.xlsx (4157 rows)\n------------------------------\nNew CSV files created: 'train_data.csv' and 'val_data.csv'\n\nColumn names found: ['id', 'text_in_author_style', 'author']\n\nFirst 2 rows of Training Data:\n     id                               text_in_author_style      author\n0  5843  هذه الكتب التي أصدرتُها منذ بدأت كتابة باب \"من...  يوسف إدريس\n1  5844  صعب جِدًّا في ظل هذا التقسيم الإرهابي أن أقول ...  يوسف إدريس\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"Data Preprocessing","metadata":{}},{"cell_type":"code","source":"import re\nimport string\n\ndef clean_arabic_text(text):\n    # Convert to string to avoid errors with non-text cells\n    text = str(text)\n    \n    # 1. Remove URLs, Emails, and Mentions\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    text = re.sub(r'\\S+@\\S+', '', text)\n    text = re.sub(r'@\\S+', '', text)\n    \n    # 2. Normalize Arabic letters (Alef, Yeh, and Teh Marbuta)\n    # This helps the model see different spellings of the same word as one\n    text = re.sub(\"[إأآا]\", \"ا\", text)\n    text = re.sub(\"ى\", \"ي\", text)\n    text = re.sub(\"ة\", \"ه\", text)\n    \n    # 3. Remove Arabic Diacritics (Tashkeel)\n    tashkeel_pattern = re.compile(r\"[\\u064B-\\u0652]\")\n    text = re.sub(tashkeel_pattern, \"\", text)\n    \n    # 4. Remove Tatweel (Kashida) like \"جميــــل\" -> \"جميل\"\n    text = re.sub(r'ـ+', '', text)\n    \n    # 5. Remove Punctuation (Standard and Arabic)\n    arabic_punctuation = '«»\"\\'()[]{}،؛؟!'\n    all_punct = string.punctuation + arabic_punctuation\n    text = text.translate(str.maketrans('', '', all_punct))\n    \n    # 6. Remove extra whitespaces\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text\n\n# Apply the cleaning to the training and validation dataframes\nprint(\"Starting Preprocessing... this might take a minute.\")\ntrain_df['text_in_author_style'] = train_df['text_in_author_style'].apply(clean_arabic_text)\nval_df['text_in_author_style'] = val_df['text_in_author_style'].apply(clean_arabic_text)\n\nprint(\"Status: Preprocessing Complete!\")\nprint(\"\\nSample of Cleaned Text (First 150 characters):\")\nprint(train_df['text_in_author_style'].iloc[0][:150])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T21:04:21.637858Z","iopub.execute_input":"2026-01-09T21:04:21.638304Z","iopub.status.idle":"2026-01-09T21:04:50.782844Z","shell.execute_reply.started":"2026-01-09T21:04:21.638271Z","shell.execute_reply":"2026-01-09T21:04:50.781574Z"}},"outputs":[{"name":"stdout","text":"Starting Preprocessing... this might take a minute.\nStatus: Preprocessing Complete!\n\nSample of Cleaned Text (First 150 characters):\nهذه الكتب التي اصدرتها منذ بدات كتابه باب من مفكره يوسف ادريس في الاهرام كل اثنين القت علي شخصيا وعلي الحركه الثقافيه والفنيه المصريه والعربيه سؤالا ل\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"Label Encoding (Assigning IDs to Authors)","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nimport json\n\n# 1. Initialize the LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# 2. Fit and transform the 'author' column to numbers\ntrain_df['label'] = label_encoder.fit_transform(train_df['author'])\nval_df['label'] = label_encoder.transform(val_df['author'])\n\n# 3. Create a dictionary to remember which number belongs to which author\nid2label = {int(i): label for i, label in enumerate(label_encoder.classes_)}\nlabel2id = {label: int(i) for i, label in enumerate(label_encoder.classes_)}\n\n# 4. Save the mapping to a file (important for the testing phase later)\nwith open('mapping.json', 'w', encoding='utf-8') as f:\n    json.dump(id2label, f, ensure_ascii=False)\n\nprint(\"Status: Success\")\nprint(f\"Total Unique Authors: {len(label_encoder.classes_)}\")\nprint(\"-\" * 30)\nprint(\"Samples of the mapping (ID to Author Name):\")\nfor i in range(min(5, len(label_encoder.classes_))):\n    print(f\"ID {i}  ==>  {id2label[i]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T21:04:56.361098Z","iopub.execute_input":"2026-01-09T21:04:56.361589Z","iopub.status.idle":"2026-01-09T21:04:57.104092Z","shell.execute_reply.started":"2026-01-09T21:04:56.361557Z","shell.execute_reply":"2026-01-09T21:04:57.103009Z"}},"outputs":[{"name":"stdout","text":"Status: Success\nTotal Unique Authors: 21\n------------------------------\nSamples of the mapping (ID to Author Name):\nID 0  ==>  أحمد أمين\nID 1  ==>  أحمد تيمور باشا\nID 2  ==>  أحمد شوقي\nID 3  ==>  أمين الريحاني\nID 4  ==>  ثروت أباظة\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"******Tokenization & Sliding Window******","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer\n\n# 1. Load the MARBERTv2 Tokenizer\nmodel_name = \"UBC-NLP/MARBERTv2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef tokenize_with_sliding_window(df, max_len=512, stride=256):\n    all_input_ids = []\n    all_attention_masks = []\n    all_labels = []\n    \n    print(\"Processing texts... this might take a few minutes.\")\n    \n    for index, row in df.iterrows():\n        text = str(row['text_in_author_style'])\n        label = row['label']\n        \n        # Tokenize the whole text without truncation first\n        full_tokens = tokenizer(text, truncation=False, add_special_tokens=True)\n        input_ids = full_tokens['input_ids']\n        \n        # If text is short, just pad it\n        if len(input_ids) <= max_len:\n            chunks = [input_ids]\n        else:\n            # Create overlapping chunks\n            chunks = [input_ids[i : i + max_len] for i in range(0, len(input_ids), stride)]\n        \n        for chunk in chunks:\n            # Padding if the last chunk is smaller than max_len\n            if len(chunk) < max_len:\n                chunk = chunk + [tokenizer.pad_token_id] * (max_len - len(chunk))\n            else:\n                chunk = chunk[:max_len]\n            \n            mask = [1 if t != tokenizer.pad_token_id else 0 for t in chunk]\n            \n            all_input_ids.append(chunk)\n            all_attention_masks.append(mask)\n            all_labels.append(label)\n            \n    return {\n        'input_ids': torch.tensor(all_input_ids),\n        'attention_mask': torch.tensor(all_attention_masks),\n        'labels': torch.tensor(all_labels)\n    }\n\n# Apply to Train and Validation\ntrain_data_final = tokenize_with_sliding_window(train_df)\nval_data_final = tokenize_with_sliding_window(val_df)\n\nprint(f\"\\nStatus: Success\")\nprint(f\"Original Training Rows: {len(train_df)}\")\nprint(f\"Final Training Samples (after sliding window): {len(train_data_final['labels'])}\")\nprint(f\"Final Validation Samples: {len(val_data_final['labels'])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T21:05:06.666806Z","iopub.execute_input":"2026-01-09T21:05:06.667355Z","iopub.status.idle":"2026-01-09T21:08:21.243406Z","shell.execute_reply.started":"2026-01-09T21:05:06.667319Z","shell.execute_reply":"2026-01-09T21:08:21.242122Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/439 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a973beea0614058a053f70edb954077"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8eddabc7fece42e89d2d10d034ed176d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1d961999336485cbe65df90a3aabc1c"}},"metadata":{}},{"name":"stdout","text":"Processing texts... this might take a few minutes.\nProcessing texts... this might take a few minutes.\n\nStatus: Success\nOriginal Training Rows: 35122\nFinal Training Samples (after sliding window): 35658\nFinal Validation Samples: 4208\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"Formatting Data for the Model****","metadata":{}},{"cell_type":"code","source":"import torch\n\nclass AuthorDataset(torch.utils.data.Dataset):\n    def __init__(self, data_dict):\n        # We store the dictionary of tensors (input_ids, attention_mask, labels)\n        self.data = data_dict\n        \n    def __len__(self):\n        # Return the total number of samples\n        return len(self.data['labels'])\n    \n    def __getitem__(self, idx):\n        # Return a single sample (one row) for the model to process\n        return {key: val[idx] for key, val in self.data.items()}\n\n# Create the final dataset objects\ntrain_dataset = AuthorDataset(train_data_final)\nval_dataset = AuthorDataset(val_data_final)\n\nprint(\"Status: Success\")\nprint(f\"Train Dataset: {len(train_dataset)} samples ready.\")\nprint(f\"Validation Dataset: {len(val_dataset)} samples ready.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T21:08:41.577863Z","iopub.execute_input":"2026-01-09T21:08:41.578829Z","iopub.status.idle":"2026-01-09T21:08:41.589540Z","shell.execute_reply.started":"2026-01-09T21:08:41.578785Z","shell.execute_reply":"2026-01-09T21:08:41.588378Z"}},"outputs":[{"name":"stdout","text":"Status: Success\nTrain Dataset: 35658 samples ready.\nValidation Dataset: 4208 samples ready.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"Starting the Training Engine","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport numpy as np\nfrom sklearn.metrics import f1_score, accuracy_score\n\n# 1. Load MARBERTv2\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"UBC-NLP/MARBERTv2\", \n    num_labels=21\n)\n\n# 2. Define Metrics\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    f1 = f1_score(labels, predictions, average='macro')\n    acc = accuracy_score(labels, predictions)\n    return {\"macro_f1\": f1, \"accuracy\": acc}\n\n# 3. Training Configurations (Updated for compatibility)\ntraining_args = TrainingArguments(\n    output_dir='./author_id_model',\n    num_train_epochs=3,\n    per_device_train_batch_size=8,   # Reduced to 8 to be safer\n    per_device_eval_batch_size=8,\n    learning_rate=2e-5,\n    eval_strategy=\"epoch\",           # Changed from evaluation_strategy\n    save_strategy=\"epoch\",\n    logging_steps=100,\n    fp16=True,                       # GPU acceleration\n    load_best_model_at_end=True,\n    metric_for_best_model=\"macro_f1\",\n    report_to=\"none\"\n)\n\n# 4. Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n)\n\n# 5. EXECUTE\nprint(\"Starting training with updated settings...\")\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom tqdm.auto import tqdm\n\n# 1. Load the trained model and tokenizer\nmodel_path = \"./author_id_model\" # This is where the trainer saved it\ntokenizer = AutoTokenizer.from_pretrained(\"UBC-NLP/MARBERTv2\")\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\nmodel.to(\"cuda\") # Move to GPU for fast prediction\nmodel.eval()\n\n# 2. Load the Competition Test Data\ntest_df = pd.read_excel(\"/kaggle/input/authorid/AuthorshipClassficiationTest.xlsx\")\n\n# 3. Clean the test text (same way we did in training)\ntest_df['cleaned_text'] = test_df['text_in_author_style'].str.replace(r'[إأآا]', 'ا', regex=True).str.replace(r'ة', 'ه', regex=True)\n\npredictions = []\n\n# 4. Start Predicting\nprint(\"Predicting authors for the test set...\")\nwith torch.no_grad():\n    for text in tqdm(test_df['cleaned_text']):\n        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(\"cuda\")\n        outputs = model(**inputs)\n        pred_id = torch.argmax(outputs.logits, dim=-1).item()\n        # Convert ID back to Author Name using our map from Cell 3\n        author_name = id2author[pred_id]\n        predictions.append(author_name)\n\n# 5. Create the Submission File\nsubmission = pd.DataFrame({\n    'id': test_df['id'],\n    'author': predictions\n})\n\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Success! Your submission.csv is ready.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T23:41:53.408176Z","iopub.execute_input":"2026-01-09T23:41:53.408512Z","iopub.status.idle":"2026-01-09T23:42:13.393010Z","shell.execute_reply.started":"2026-01-09T23:41:53.408483Z","shell.execute_reply":"2026-01-09T23:42:13.391348Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/439 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af7582c4ec364d2f9df59ff2c6f4abd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e509c81910a4a418f9ca1c492dfefb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ce80c63f69b4217b2ac595b7f93764e"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    480\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mREPO_ID_REGEX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0;34m\"Repo id must use alphanumeric chars, '-', '_' or '.'.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: './author_id_model'.","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/2386953783.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./author_id_model\"\u001b[0m \u001b[0;31m# This is where the trainer saved it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"UBC-NLP/MARBERTv2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Move to GPU for fast prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m                 \u001b[0;31m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    509\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m                     \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \"\"\"\n\u001b[0;32m--> 322\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;31m# Now we try to recover if we can find all files correctly in the cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         resolved_files = [\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0m_get_cache_file_to_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfull_filenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         ]\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36m_get_cache_file_to_return\u001b[0;34m(path_or_repo_id, full_filename, cache_dir, revision, repo_type)\u001b[0m\n\u001b[1;32m    141\u001b[0m ):\n\u001b[1;32m    142\u001b[0m     \u001b[0;31m# We try to see if we have a cached version (not up to date):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     resolved_file = try_to_load_from_cache(\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         ):\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"token\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marg_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mREPO_ID_REGEX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0;34m\"Repo id must use alphanumeric chars, '-', '_' or '.'.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;34m\" The name cannot start or end with '-' or '.' and the maximum length is 96:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: './author_id_model'."],"ename":"HFValidationError","evalue":"Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: './author_id_model'.","output_type":"error"}],"execution_count":1}]}